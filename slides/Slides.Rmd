
---
title: "Predictive Modeling Process"
author: Joseph Francia and Nicholas Saber
date: November 4, 2016
output: ioslides_presentation
---


## Introduction
Our intention for this project is to evaluate the predictive power of the following five regressions: Least-Squares regression, Ridge regression, Lasso regression, Principal Component regression, and Partial Least-Squares regression.  

We will be using a dataset where the response variable is an individual's credit card balance and the explanatory variables are an individual's characteristics (gender, income, age, etc.).

## Exploratory Data Analysis
Before we ran any sort of regression on the dataset, we first engaged in some exploratory data analysis by making histograms, boxplots, and summary statistics for the variables in our dataset. Here's a boxplot of our Balance variable, which is our response variable.

<div class="centered">
<img src="images/boxplot-Balance.png" width="300" length="100" /> 
</div>

## Preparing our Data for Regression Analysis
In order for us to be able to run our regressions, we first need to modify the data so it's suitable for regression analysis. We manipulated our data in the following ways:  
  
1. Converted columns of categorical variables into numerical binary columns  
  
2. Centered and standardized the dataset  
  
3. Divided the data into training and test sets

Now, we're ready to run our regressions on our dataset. 

##Methodology (Part 1)
After dividing our data into training and test sets, we're now ready to run our regressions. These are the five regressions that we will be analyzing for this project:  
  
  
1. Least Squares Regression  
2. Ridge Regression  
3. Lasso Regression  
4. Partial Least Squares Regression  
5. Principal Component Regression  

##Methodology (Part 2) 
Lets go through a quick theoretical overview for each regression method.  
  
Least Squares: solution is unbiased, but has high variance  
  
Ridge: solution minimizes both residuals and squared norm of beta vector  
  
Lasso: solution minimizes both residuals and absolute norm of beta vector  

PCA: solution is a result of running a regression on linear combinations of explanatory variable columns  
  
PLSR: similar to PCA, but its linear combinations of explanatory variables take into account the strength of each explanatory variable's relationship with the response variable. 


##Regression Coefficients
Lets take a look at how our regression coefficients differed for each regression type. The plot below demonstrates how the regression coefficients change for each regression type: 
<div class="centered">
<img src="images/coefficient_plot.png" width="250" length="100" /> 
</div>
While informative, this coefficient plot doesn't tell us about the predictive performance of our regressions. In the next slide, I'll go over how well/poorly our regressions did from a prediction standpoint.  

##Regression Prediction Performance

```{r results='asis', echo=FALSE,warning=FALSE}
  library(xtable)
  load(file="../data/mse_table.RData")
  x=xtable(mse, digits=6)
  print(x, type="html", comment=F)
```
<p>&nbsp; 

As one can see, least squares regression performed the best, and its average squared error was a mere .047. Principal component regression, meanwhile, performed the worst by far, and it had an MSE of .31. 

##Conclusion 

While least-squares regression seems like it would be the best regression method to use for prediction purposes, this is probably only be the case for this dataset. Ridge and Lasso, thanks to their penalty on model complexity, actually usually outperform least-squares prediction in prediction. As for the dimension-reduction regression methods, Principal component regression is significantly inferior to partial least squares regression. This is likely because, unlike principal component regression, partial least squares regression takes into account how each explanatory variable interacts with the response variable. 
