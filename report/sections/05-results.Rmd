---
output: pdf_document
---

#Results

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Before we look at regression performance, lets take a look at how the estimated regression coefficients differ for each regression: 

```{r, fig.width=50, echo =FALSE,warning=FALSE, message=FALSE}
library(xtable)
library(ggplot2)
load(file="../../data/betas.RData")
knitr::kable(betas, digits = 3, caption = 'A table of coefficients', width=50)
```


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As one can see with the above table and plot, Lasso Regression provides us a sparse estimator. In other words, Lasso Regression provides a solution in which multiple beta coefficients are zero. Although Ridge does not provide a sparse solution, it does tend to shrink the beta coefficients towards zero. This is why both Lasso and Ridge are called shrinkage methods.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Thanks to the above visual, it's also easy to tell that there are four variables that have a strong relationship with the response variable (Credit Card Balance). These variables have beta coefficients that are large in magnitude and they are Income, Limit, Rating, and Student. The Limit, Rating, and Student variables are strongly positively correlated with Credit Card Balance while the Income variable is significantly negatively correlated with Credit Card Balance. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now that we have a better understanding of each of these regression methods, lets look at the data in order to see how each regression performed: 

```{r,echo=FALSE,message=FALSE,warning=FALSE,errors=FALSE}
load("../../data/mse_table.RData")
barplot(t(mse), xaxt="n", main="MSE of Regression Methods",fin=c(3,3))
axis(side=1,labels=c("OLS", "Ridge", "PLS","PCR","Lasso"), at=c(.75,1.95,3.15,4.35,5.55),fin=c(1,1))
```
![Coefficient Plot](images/coefficient_plot.png)

