---
title: "03-methods.Rmd"
output: html_document
---
#Methods

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; After dividing our data into training and test sets, we're now ready to run our regressions. Before we start, however, I want to give a brief theoretical overview of each regression method.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Least squares regression is the most basic form of regression. The solution to least-squares regression minimizes the sum of squared residuals. While this solution is unbiased, it turns out that this solution isn't optimal for prediction.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thus, we turn to modifications of least squares regression: Ridge Regression and Lasso Regression. The solutions to both of these regression problems are biased estimators that have lower variance than the least-squares solution. In Lasso regression, the solution is the beta vector that minimizes both the sum of squared residuals (bias) and the **absolute** norm of the beta vector (model complexity). In Ridge regression, meanwhile, its solution is the beta vector that minimizes both the sum of squared residuals and the **squared** norm of the beta vector. By penalizing and shrinking the length of the beta vector, both Ridge and Lasso Regression are considered shrinkage methods. By finding a solution that minimizes **both** variance (model complexity) and bias (residuals), these biased estimators perform better from a prediction standpoint than the least-squares estimator.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; There are, however, more alternatives to least-squares regression.  Lets take a look at Principal Component Regression (PCR) and Partial Least Squares Regression (PLSR). These methods are known as dimension-reduction methods. Unlike shrinkage methods, PLSR and PCR first reduce the column/dimension size of the dataset by summarizing the dataset with fewer columns. The columns of this reduced dataset consist of linear combinations of columns from the original dataset. Then, a model is fit onto these newly formed columns. It turns out that these shrinkage models can actually [outperform](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf) least-squares regression. The difference between PLSR and PCR is how they go about forming the columns of the reduced/summarized dataset. PCR forms the columns of its reduced dataset finding linear combinations of its predictors that best represent the original dataset. PLSR, meanwhile, is similar to PCR, but it also incorporates the response variable into its computation. In other words, when forming its summarized dataset, PLSR takes into account how well each predictor column explains the response variable.